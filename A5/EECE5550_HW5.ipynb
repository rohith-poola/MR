{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW5: Simultaneous Localization & Mapping (SLAM)\n",
        "\n",
        "## EECE 5550: Mobile Robotics (Spring 2025)"
      ],
      "metadata": {
        "id": "I0dAV0uFEb-R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gMxVX7yaLr"
      },
      "source": [
        "**Collaboration Statement:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXaBB3IJyxcb"
      },
      "outputs": [],
      "source": [
        "# Fill this in per the syllabus, or we will assign a zero to this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Q-SupywN0g"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaX3MKJZiWb3"
      },
      "source": [
        "This semester, we will use a custom simulator, called `gym-neu-racing`, to develop navigation algorithms. We implemented the basic structure of this simulator for you, and the HW assignments will ask you to implement important functions (e.g., kinematics, sensing, planning, mapping).\n",
        "\n",
        "To install the simulator, you can use this command (it will download the latest code from GitLab and automatically install it in the environment your Colab notebook runs in):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1a17fpJ9ONw"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://gitlab.com/neu-autonomy/gym-neu-racing.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtsam"
      ],
      "metadata": {
        "id": "uos2qHrn43TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U528T-DbjK3L"
      },
      "source": [
        "Now that the simulator and its dependencies have been installed, you can import the modules you'll need for this assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylskOzCW-VEt"
      },
      "outputs": [],
      "source": [
        "import gymnasium\n",
        "import numpy as np\n",
        "import gym_neu_racing\n",
        "from gymnasium import spaces\n",
        "from gym_neu_racing.envs.wrappers import StateFeedbackWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable, Optional\n",
        "import matplotlib.cm as cmx\n",
        "import matplotlib.colors as colors\n",
        "from gym_neu_racing import motion_models\n",
        "from gym_neu_racing import sensor_models\n",
        "from gym_neu_racing.sensor_models.sensor_model import SensorModel\n",
        "from typing import Iterable, Optional\n",
        "import gtsam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gtsam import Marginals, Point2, Point3, Pose2, Pose3, Values\n",
        "from gtsam.symbol_shorthand import L, X\n",
        "from matplotlib import patches\n",
        "from scipy.linalg import expm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Point Cloud Registration with ICP"
      ],
      "metadata": {
        "id": "plrBWYS7opHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll impelement the iterative closest point (ICP) algorithm in this problem. You will be given two pointclouds, `X, Y`, which contain points in $R^n$, and an initial guess of the transformation, in the form of a translation vector `t` $\\in R^n$ and rotation matrix `R` $\\in SO(n)$. You should write your code so it can handle both $n=2$ and $n=3$ seamlessly. And `X` and `Y` may not contain the same number of points!\n",
        "\n",
        "This algorithm will be important for doing Lidar Odometry & SLAM in Problem 2 below, where you'll run ICP between pointclouds from the lidar sensor as the robot moves around the environment."
      ],
      "metadata": {
        "id": "GWHqAWx_1xwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1a) Estimate correspondences, given a guess of the transformation\n",
        "\n",
        "Recall that ICP begins with an initial guess of the transformation between two pointclouds.\n",
        "\n",
        "To be able to use Horn's method to improve that initial guess, we need to know which points in X correspond to points in Y. In ICP, we'll use the current guess of the transformation to help estimate these correspondences. These correspondences likely won't all be correct, but they will hopefully at least be good enough to improve the transformation estimate, which could lead to better correspondences in the next iteration.\n",
        "\n",
        "In this part, you should implement `estimate_correspondences` to return a list of tuples that represent correspondences. That is, let each element `(i,j)` of C denote that after transforming the `i`th element from X by `R` and `t`, the closest point in Y has index `j`, and that closest point is within `d_max` ($l_2$ distance). Think about what happens if you make `d_max` too big or small -- it's an important parameter!\n",
        "\n",
        "You are encouraged to make this calculation reasonably efficient, because ICP will call this at each iteration, and your SLAM code will call ICP many times! For example, an initial implementation that loops through every pair in X and Y may give the right answer but could be made much much faster by vectorizing the code and/or using numpy operations, such as `np.argmin` and `np.linalg.norm`.\n",
        "\n",
        "**Devlierables**:\n",
        "- Implement `estimate_correspondences` below to take in the two pointclouds `X, Y` and an estimated transformation, `R, t`, and output a list of correspondences, `C`"
      ],
      "metadata": {
        "id": "sleasHEwosay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ICP:\n",
        "    def __init__(self, d_max=0.5, num_iterations=100, epsilon=1e-4):\n",
        "        self.d_max = d_max\n",
        "        self.num_iterations = num_iterations\n",
        "        self.epsilon = 1e-4\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        Y: np.ndarray,\n",
        "        t0: np.ndarray,\n",
        "        R0: np.ndarray,\n",
        "        plot_every=np.inf,\n",
        "        show=False,\n",
        "        pause=False,\n",
        "        verbose=False,\n",
        "    ) -> tuple[np.ndarray, np.ndarray]:\n",
        "        # estimate t,R s.t. ||Y - (R @ x + t)|| is minimized\n",
        "        # --> rotate x so it looks like y\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return t, R\n",
        "\n",
        "    def estimate_correspondences(self, X: np.ndarray, Y: np.ndarray, t: np.ndarray, R: np.ndarray) -> list[tuple[int, int]]:\n",
        "        # C is a list of tuples.\n",
        "        # If index i in Rx+t has closest point in Y with index j (and within\n",
        "        # d_max), then add (i,j) to C.\n",
        "        C = []\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return C\n",
        "\n",
        "    def compute_optimal_rigid_registration(self, X: np.ndarray, Y: np.ndarray, C: list) -> tuple[np.ndarray, np.ndarray]:\n",
        "        # Horn's method from lecture to get t, R from X, Y, C\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return t, R\n",
        "\n",
        "def RMSE(X: np.ndarray, Y: np.ndarray, C: list[tuple[int, int]], t: np.ndarray, R: np.ndarray) -> float:\n",
        "    # Of the corresponding points, calculate root mean square error btwn\n",
        "    # Y and the transformed X\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "sizHJKhPpN0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1b) Compute the optimal transformation, given a guess of the correspondences\n",
        "\n",
        "Now that you have a guess of the correspondences, you can calculate the optimal (w.r.t. those possibly incorrect correspondences) rigid transformation between the two pointclouds using Horn's method.\n",
        "\n",
        "**Deliverables:**\n",
        "- Implement `compute_optimal_rigid_registration` above, which takes in the two pointclouds `X, Y` and correspondences `C`, and outputs the translation vector and rotation matrix, `t, R`"
      ],
      "metadata": {
        "id": "DKVMGZzjtO1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1c) Calculate the RMSE of the current transformation estimate\n",
        "\n",
        "You need a way to estimate how good the current transformation estimate is, to determine if your algorithm has converged to a local minimum. In this part, you should calculate the root mean square error (RMSE) between the transformed pointcloud X and the pointcloud Y, *among the corresponding pairs of points*.\n",
        "\n",
        "**Deliverables**:\n",
        "- Implement `RMSE` above, which takes in the pointcouds `X, Y`, correspondences `C`, and transformation `t, R`, and outputs the RMSE (scalar). Note that this is a function outside of the `ICP` class (no `self` argument)."
      ],
      "metadata": {
        "id": "SqTw1_mPv2WY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1d) Iterate between estimating the corresponences and transformation, until convergence\n",
        "\n",
        "To complete the ICP implementation, simply iterate between the two methods you just implemented. You should add two termination conditions: terminate after a maximum number of iterations, or if the RMSE between successive iterations has not decreased by at least $\\epsilon$.\n",
        "\n",
        "**Deliverables**:\n",
        "- Implement `run` above, which takes in the pointclouds `X, Y`, an initial guess of the transformation, `t0, R0`, and outputs a final transformation `t, R`. This method calculates the `t, R` to minimize the error between $Y$ and the transformed pointcloud, $RX+t$.\n",
        "- Provide a plot of the two pointclouds at the beginning and end of the iterations\n",
        "- Get familiar with what happens if the initial guess is not very good, and how the various parameters affect your result (this is not a deliverable per se, but having this intuition will help a lot in Problem 2)"
      ],
      "metadata": {
        "id": "SKVtonLTueWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see if your ICP is working, you can use the following code. Here are some plotting utilities, you're welcome to modify or re-write:"
      ],
      "metadata": {
        "id": "6Rf-UMW31P9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot3D(X, Y, t, R, C=None, show=True, pause=False):\n",
        "    ax = plt.gca()\n",
        "    plt.cla()\n",
        "    ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], c=\"b\", marker=\".\")\n",
        "    Xrot = np.dot(R, X.T).T + t\n",
        "    ax.scatter(Xrot[:, 0], Xrot[:, 1], Xrot[:, 2], c=\"r\", marker=\".\")\n",
        "    plt.axis(\"equal\")\n",
        "    if show:\n",
        "        plt.show()\n",
        "    elif pause:\n",
        "        plt.pause(pause)\n",
        "\n",
        "\n",
        "def plot2D(X, Y, t, R, C=None, show=True, pause=False):\n",
        "    ax = plt.gca()\n",
        "    plt.cla()\n",
        "\n",
        "    # show the ptcloud Y and the transformed X (using R, t)\n",
        "    ax.scatter(Y[:, 0], Y[:, 1], c=\"b\", marker=\".\")\n",
        "    Xrot = np.dot(R, X.T).T + t\n",
        "    ax.scatter(Xrot[:, 0], Xrot[:, 1], c=\"r\", marker=\".\")\n",
        "\n",
        "    # show a line between correspondences, if provided\n",
        "    if C is not None:\n",
        "        for c in C:\n",
        "            x = Xrot[c[0]]\n",
        "            y = Y[c[1]]\n",
        "            ax.plot([x[0], y[0]], [x[1], y[1]], \"k-\")\n",
        "    plt.axis(\"equal\")\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    elif pause:\n",
        "        plt.pause(pause)\n",
        "\n",
        "\n",
        "def plot(X, Y, t, R, C=None, show=True, pause=False):\n",
        "    d = X.shape[1]\n",
        "    if d == 2:\n",
        "        plot2D(X, Y, t, R, C=C, show=show, pause=pause)\n",
        "    elif d == 3:\n",
        "        plot3D(X, Y, t, R, C=C, show=show, pause=pause)"
      ],
      "metadata": {
        "id": "-Qqm3i1JY8Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the two 2D pointclouds [from here](https://northeastern-my.sharepoint.com/:f:/g/personal/m_everett_northeastern_edu/EqPcPbQGbH9Ltb5jzdKeKiIBqBkvQfIsxgThTliRFMI79w?e=JNO08h) and upload them to Colab or update the paths below if working locally.\n",
        "\n",
        "Here is a block to load two pointclouds from .txt files, give an initial guess, and run ICP using parameters you can adjust:"
      ],
      "metadata": {
        "id": "fKVYezSVeSvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2d example\n",
        "d = 2\n",
        "\n",
        "# load pointclouds from file, and downsample to only load every n-th pt\n",
        "# --> feel free to increase 5 to a larger number to get a sparser pointcloud\n",
        "# while you're tuning! That should make ICP and plotting much faster.\n",
        "X = np.loadtxt(\"logo_pcl2dX.txt\", usecols=range(d))[::5]\n",
        "Y = np.loadtxt(\"logo_pcl2dY.txt\", usecols=range(d))[::5]\n",
        "\n",
        "# initial guess\n",
        "theta = -0.8\n",
        "t0 = np.array([-300, 400])\n",
        "R0 = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "\n",
        "# run ICP (adjust the parameters as needed!)\n",
        "d_max = 10\n",
        "num_iterations = 50\n",
        "registrar = ICP(d_max=d_max, num_iterations=num_iterations)\n",
        "t, R = registrar.run(X, Y, t0, R0, plot_every=1, show=True)\n",
        "print(f\"{t=}\")\n",
        "print(f\"{R=}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "otUDOTsM1OrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to try this with other pointclouds from your own images for fun, we provide some code in the Appendix of this notebook."
      ],
      "metadata": {
        "id": "Pb4n0nfj8XVo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKUvYLxpv2p4"
      },
      "source": [
        "# Problem 2: Lidar SLAM with Factor Graphs and ICP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous assignment, you were given either the robot's true state *or* the true map. In this problem, you'll implement a basic version of SLAM, where you only have access to the robot's control inputs and measurements (not the true state or map).\n",
        "\n",
        "The setup is as follows:\n",
        "- The robot will start at some initial state and follow along a pre-defined trajectory to explore the environment. No need to worry about the planner, unless you want to do the extra credit at the very end (**2e**).\n",
        "- The robot uses a Unicycle motion model and has a lidar sensor and a noisy wheel speed sensor. If you just accumulate the noisy wheel speed measurements according to the motion model, you may get a mildly useful map for small noise magnitudes but the map will become a smeared mess for larger noise magnitudes (**2b**).\n",
        "- You can use your ICP implementation above to improve the estimate from purely accumulating the odometry. You will do this by adding relative pose factors between successive timesteps, which should improve the results by a lot (**2c**). To get a globally consistent map, you should also add some loop closures, which can be done in this assignment by hard-coding pairs of timesteps at which to run an additional round of ICP (**2d**).\n",
        "- The environment is 10x10m and has walls all around the perimeter.\n",
        "- You should do all of your estimation in the robot's initial body coordinate frame. That is, your initial pose estimate will be $(x, y, \\theta) = (0, 0, 0)$ and subsequent poses/maps will be calculated relative to that. Meanwhile, the simulator will express the robot's true state in its own world frame (e.g., `env.unwrapped.state` will be `np.array([0.5, -3.5, np.pi / 2])`). To account for this static transform and make your visualizations more intuitive, we apply the initial pose transformation for you when rendering the map. Feel free to undo this if it's confusing (pass `initial_state=None` when `compute_map` is called below)."
      ],
      "metadata": {
        "id": "UUrJfA75GiFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions / Background"
      ],
      "metadata": {
        "id": "a5RkI8-647AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a SLAM system turns out to require a decent amount of code. To let you focus on the factor graph components, we provide a handful of helper functions you can use, ignore, or re-write as needed.\n",
        "\n",
        "We defined a simple open-loop control policy that will send the robot in a path around the environment to get reasonably good viewpoints with a few opportunities for loop closures. This policy hard-coded to this environment and initial state, which is not great practice but will let you tune your SLAM algorithm in a repeatable way. You should be able to make a good (albeit not 100% complete) map using this policy. For extra credit, you will have a chance to add a closed-loop policy that actively explores the environment as quickly as possible."
      ],
      "metadata": {
        "id": "Wij4MftC9489"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenLoopPolicy:\n",
        "    def __init__(self):\n",
        "        self.actions = [\n",
        "            (4., np.array([1.0, 0.0])),\n",
        "            (5., np.array([0.5, -np.pi / 2])),\n",
        "            (8., np.array([1.0, 0.0])),\n",
        "            (9., np.array([0.5, -np.pi / 2])),\n",
        "            (14., np.array([1.0, 0.0])),\n",
        "            (15., np.array([0.5, -np.pi / 2])),\n",
        "            (19.5, np.array([1.0, 0.0])),\n",
        "            (20.5, np.array([0.5, -np.pi / 2])),\n",
        "            (28.5, np.array([1.0, 0.0])),\n",
        "            (29.5, np.array([0.5, np.pi / 2])),\n",
        "            (32., np.array([1.0, 0.0])),\n",
        "            (33., np.array([0.5, np.pi / 2])),\n",
        "            (50., np.array([1.0, 0.0])),\n",
        "        ]\n",
        "        self.current_action_index = 0\n",
        "\n",
        "    def get_action(self, t: float) -> np.ndarray:\n",
        "        # Command the speed in the 2nd element until time exceeds 1st element,\n",
        "        # then move to the next row in self.actions\n",
        "        current_action = self.actions[self.current_action_index]\n",
        "        if t > current_action[0]:\n",
        "            self.current_action_index += 1\n",
        "        action_twist = current_action[1]\n",
        "        return action_twist"
      ],
      "metadata": {
        "id": "HDFA0O8I94DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your LidarOdometry will estimate the robot's pose at each timestep. To turn that into a map of the environment, you can use my `Mapper` implementation from the last assignment. The `compute_mapper` function will take in the list of past lidar measurements and estimated robot poses, and generates a map of the environment. If you have a better/faster way of going from measurements and estimated poses to a map, feel free to use that instead! This code runs pretty fast for short trajectories but will take a second or so to generate a map once the trajectory gets above ~100 steps."
      ],
      "metadata": {
        "id": "8mW8y5m0-zE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gym_neu_racing.envs.map import Map\n",
        "from skimage.draw import line\n",
        "\n",
        "\n",
        "def log_odds_to_prob(log_odds):\n",
        "    return 1 - 1 / (1 + np.exp(log_odds))\n",
        "\n",
        "\n",
        "def prob_to_log_odds(prob):\n",
        "    return np.log(prob / (1 - prob))\n",
        "\n",
        "\n",
        "class Mapper:\n",
        "    def __init__(\n",
        "        self,\n",
        "        p_free: float = 0.2,\n",
        "        p_occ: float = 0.8,\n",
        "        p_prior: float = 0.5,\n",
        "        x_width=10,\n",
        "        y_width=10,\n",
        "        grid_cell_size=0.1,\n",
        "    ):\n",
        "        self.map = Map(\n",
        "            x_width=x_width, y_width=y_width, grid_cell_size=grid_cell_size\n",
        "        )\n",
        "\n",
        "        self.p_free = p_free\n",
        "        self.p_occ = p_occ\n",
        "        self.p_prior = p_prior\n",
        "        self.log_odds_free = prob_to_log_odds(p_free)\n",
        "        self.log_odds_occ = prob_to_log_odds(p_occ)\n",
        "        self.log_odds_prior = prob_to_log_odds(p_prior)\n",
        "\n",
        "        # initialize with P(occ) = 0.5 for all cells (unknown)\n",
        "        self.map.log_odds_map = np.zeros(\n",
        "            self.map.static_map.shape, dtype=float\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def probability_map(self):\n",
        "        return log_odds_to_prob(self.map.log_odds_map)\n",
        "\n",
        "    def step(self, obs):\n",
        "        \"\"\"Using current state & lidar, update esimated gridmap log-odds.\"\"\"\n",
        "\n",
        "        # Use angles/ranges to get the (x, y) coordinates in the lidar frame, but\n",
        "        # skip any pts where the range = np.inf\n",
        "        ranges = obs[\"lidar\"][\"ranges\"]\n",
        "        non_max_range_inds = np.where(ranges < np.inf)\n",
        "        ranges = ranges[non_max_range_inds]\n",
        "        angles = obs[\"lidar\"][\"angles\"][non_max_range_inds]\n",
        "        hit_pts_in_lidar_frame = np.stack(\n",
        "            [ranges * np.cos(angles), ranges * np.sin(angles)]\n",
        "        ).T\n",
        "\n",
        "        # Build the lidar-to-world TF matrix using the robot's current position, heading\n",
        "        x, y, theta = obs[\"state\"]\n",
        "        T_world_lidar = np.array(\n",
        "            [\n",
        "                [np.cos(theta), -np.sin(theta), x],\n",
        "                [np.sin(theta), np.cos(theta), y],\n",
        "                [0, 0, 1],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Transform the (x,y) coordinates from the lidar frame to the world frame\n",
        "        hit_pts_in_lidar_frame_homogeneous = np.hstack(\n",
        "            [\n",
        "                hit_pts_in_lidar_frame,\n",
        "                np.ones((hit_pts_in_lidar_frame.shape[0], 1)),\n",
        "            ]\n",
        "        )\n",
        "        hit_pts_in_world_frame = np.dot(\n",
        "            hit_pts_in_lidar_frame_homogeneous, T_world_lidar.T\n",
        "        )[:, :2]\n",
        "\n",
        "        hit_pts_in_map_inds, in_map = (\n",
        "            self.map.world_coordinates_to_map_indices(hit_pts_in_world_frame)\n",
        "        )\n",
        "        # *** questionable logic *** ignore any lidar beams that end outside the map\n",
        "        # --> should probably include the pts within the map...\n",
        "        hit_pts_in_map_inds = hit_pts_in_map_inds[in_map == True]\n",
        "        robot_pos_in_map_inds, in_map = (\n",
        "            self.map.world_coordinates_to_map_indices(obs[\"state\"][:2])\n",
        "        )\n",
        "\n",
        "        for hit_pt in hit_pts_in_map_inds:\n",
        "            rr, cc = line(\n",
        "                robot_pos_in_map_inds[0],\n",
        "                robot_pos_in_map_inds[1],\n",
        "                hit_pt[0],\n",
        "                hit_pt[1],\n",
        "            )\n",
        "\n",
        "            free_inds = (rr[:-1], cc[:-1])\n",
        "            self.map.log_odds_map[free_inds] = (\n",
        "                self.map.log_odds_map[free_inds]\n",
        "                + self.log_odds_free\n",
        "                - self.log_odds_prior\n",
        "            )\n",
        "            occ_inds = (rr[-1], cc[-1])\n",
        "\n",
        "            self.map.log_odds_map[occ_inds] = (\n",
        "                self.map.log_odds_map[occ_inds]\n",
        "                + self.log_odds_occ\n",
        "                - self.log_odds_prior\n",
        "            )\n",
        "\n",
        "        map_estimate = self.probability_map.copy()\n",
        "\n",
        "        return map_estimate\n",
        "\n",
        "def compute_map(estimated_poses: list[dict], lidar_measurements: list[dict], map_xwidth=20, map_ywidth=20, initial_state: Optional[np.ndarray]=None, pause=0.1, show=False) -> None:\n",
        "    plt.figure(\"estimated occ grid\")\n",
        "    ax = plt.gca()\n",
        "    ax.cla()\n",
        "\n",
        "    # Transform the poses that your GTSAM code estimated in the odom frame, into\n",
        "    # the world frame defined by initial_state. You can disable this \"feature\"\n",
        "    # by passing initial_state=None, in which case your estimated_poses will\n",
        "    # be plotted in the odom frame (i.e., the robot's body frame at t=0).\n",
        "    if initial_state is not None:\n",
        "        T_O_to_world = gtsam.Pose2(*initial_state)\n",
        "        for i in range(len(estimated_poses)):\n",
        "            pose_in_O = estimated_poses[i]\n",
        "            pose_in_W = T_O_to_world * pose_in_O\n",
        "            estimated_poses[i] = pose_in_W\n",
        "\n",
        "    # Instantiate the Mapper and add every lidar measurement as if it were taken\n",
        "    # with the robot at the corresponding pose in estimated_poses\n",
        "    mapper = Mapper(x_width=map_xwidth, y_width=map_ywidth)\n",
        "    for i in range(len(estimated_poses)):\n",
        "        gtsam_pose = estimated_poses[i]\n",
        "        pose_xyt = np.array([gtsam_pose.x(), gtsam_pose.y(), gtsam_pose.theta()])\n",
        "        obs = {\"state\": pose_xyt, \"lidar\": lidar_measurements[i]}\n",
        "        map_estimate = mapper.step(obs)\n",
        "\n",
        "    # Draw the estimated map along with the estimated pose at each timestep\n",
        "    # (light green = start, dark green = end of trajectory)\n",
        "    mapper.map.static_map = map_estimate.copy()\n",
        "    ax = mapper.map.draw_map(show=False, ax=ax)\n",
        "    plt.scatter(\n",
        "        [gtsam_pose.x() for gtsam_pose in estimated_poses],\n",
        "        [gtsam_pose.y() for gtsam_pose in estimated_poses],\n",
        "        c=np.arange(len(estimated_poses)) / len(estimated_poses),\n",
        "        cmap=\"Greens\",\n",
        "    )\n",
        "    if show:\n",
        "        plt.show()\n",
        "    elif pause:\n",
        "        plt.pause(pause)"
      ],
      "metadata": {
        "id": "cZqzns7T-vxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a function to convert a lidar measurement (dict of `angles` and `ranges`) to a pointcloud. This may help you to do ICP between successive lidar measurements."
      ],
      "metadata": {
        "id": "YY_DwrpNEl1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lidar_obs_to_hit_pts(lidar_measurement: dict[str, np.ndarray]) -> np.ndarray:\n",
        "\n",
        "    # Use angles/ranges to get the (x, y) coordinates in the lidar frame, but\n",
        "    # skip any pts where the range = np.inf\n",
        "    ranges = lidar_measurement[\"ranges\"]\n",
        "    non_max_range_inds = np.where(ranges < np.inf)\n",
        "    ranges = ranges[non_max_range_inds]\n",
        "    angles = lidar_measurement[\"angles\"][non_max_range_inds]\n",
        "    hit_pts_in_lidar_frame = np.stack(\n",
        "        [ranges * np.cos(angles), ranges * np.sin(angles)]\n",
        "    ).T\n",
        "    return hit_pts_in_lidar_frame"
      ],
      "metadata": {
        "id": "bEzmV1qaEmI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's a function to convert left and right wheel speeds $(\\phi_l, \\phi_r)$ into a twist $(v, \\omega)$:"
      ],
      "metadata": {
        "id": "qm8wXP3HLuI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wheel_speed_to_twist(wheel_speeds: np.ndarray) -> np.ndarray:\n",
        "    phi_l, phi_r = wheel_speeds\n",
        "    r = 0.25\n",
        "    w = 0.5\n",
        "    xdot = (r / 2) * (phi_r + phi_l)\n",
        "    tdot = (r / w) * (phi_r - phi_l)\n",
        "    return np.array([xdot, tdot])"
      ],
      "metadata": {
        "id": "GFZXSC_gLudb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2a) See what a perfect estimator would give"
      ],
      "metadata": {
        "id": "sgx91LO7r1KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving in, here's what it would look like if you knew the true robot state at each timestep. Of course, this is just going back to the mapping problem from the last assignment. But hopefully this gets you oriented in terms of whether your code below is correctly estimating the robot's poses *and* the map.\n",
        "\n",
        "You don't need to code anything up here, just run the block below:"
      ],
      "metadata": {
        "id": "Pkztn_LVr_qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment (and set random seed so any randomness is repeatable)\n",
        "env = gymnasium.make(\"gym_neu_racing/NEUMapping-v0\")\n",
        "np.random.seed(0)\n",
        "\n",
        "# Tell the environment to use a wheel speed sensor (for odometry estimation), a\n",
        "# lidar sensor (just for mapping), and make the true state\n",
        "# available in obs (just for debugging)\n",
        "env.unwrapped.sensor_models = {\n",
        "    \"state\": sensor_models.StateFeedback(),\n",
        "    \"lidar\": sensor_models.Lidar2D(\n",
        "        env.unwrapped.map,\n",
        "        num_beams=360,\n",
        "        angle_limits=np.array([-np.pi, np.pi]),\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Set up the simulation parameters\n",
        "env.unwrapped.dt = 0.25\n",
        "env.unwrapped.motion_model = motion_models.Unicycle()\n",
        "initial_state = np.array([0.5, -3.5, np.pi / 2])\n",
        "control_policy = OpenLoopPolicy()\n",
        "\n",
        "# Reset the environment and get the first observation (state + lidar + wheel speeds)\n",
        "obs, _ = env.reset()\n",
        "env.unwrapped.state = initial_state\n",
        "obs = env.unwrapped._get_obs()  # pylint:disable=protected-access\n",
        "\n",
        "states = []\n",
        "lidar_measurements = []\n",
        "\n",
        "for step_num, t in enumerate(np.arange(0, 40, env.unwrapped.dt)):\n",
        "\n",
        "    # calculate which action to implement on this timestep\n",
        "    action_twist = control_policy.get_action(t)\n",
        "\n",
        "    # Extract sensor data\n",
        "    states.append(obs[\"state\"])\n",
        "    lidar_measurements.append(obs[\"lidar\"])\n",
        "\n",
        "    # Update the factor graph with this step's odometry info & re-solve\n",
        "    estimates = {\n",
        "        \"pose_means\": [gtsam.Pose2(pose[0], pose[1], pose[2]) for pose in states]\n",
        "    }\n",
        "\n",
        "    # Update the map with latest pose estimates & lidar scans\n",
        "    if step_num % 20 == 0 and step_num > 0:\n",
        "        compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=None, pause=0.1, map_xwidth=10, map_ywidth=10)\n",
        "\n",
        "    # Advance the simulator (i.e., robot moves and makes a new measurement)\n",
        "    obs, _, _, _, _ = env.step(action_twist)\n",
        "\n",
        "# Calculate a final map after the full trajectory\n",
        "compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=None, pause=False, show=True, map_xwidth=10, map_ywidth=10)"
      ],
      "metadata": {
        "id": "uwEIntJUsSj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHr3pCDajeQr"
      },
      "source": [
        "## 2b) Factor Graphs with Odometry from Motion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSN_vAxgrB7p"
      },
      "source": [
        "To begin, in this part you will create an `OdometryFactorGraph` that estimates the robot's pose over time, using control inputs and the `Unicycle` motion model. Meanwhile, the simulator itself (the \"true robot\") will be reporting its left and right wheel speeds that have been corrupted with noise. By defining a factor graph and solving the corresponding inference problem, you will obtain an estimate of the pose (i.e., its mean and covariance at each timestep). Your `step` method will return this estimate, and we provide some functions for plotting this and generating a map based on that estimate.\n",
        "\n",
        "To set up your factor graph, you'll need to implement a few methods:\n",
        "- `add_prior_pose_factor`: add a `PriorFactorPose2` at the origin to encode where the robot starts\n",
        "- `add_motion_model_factor`: add a `BetweenFactorPose2` with the relative transformation between the current pose and previous pose\n",
        "- `solve`: find the optimal estimates for each variable node ($\\Theta_j$ in lecture) using the cost function that your factor graph defines\n",
        "\n",
        "We implemented `step` for you, but you are welcome to modify that as well.\n",
        "\n",
        "**Deliverables**:\n",
        "- Implement the three methods listed above\n",
        "- Generate a plot of your robot's estimated pose over time and the rendered map, for a few different values of the wheel speed sensor's noise variance. To calibrate your expectations, it should look perfect with `0`, it should look nearly perfect with `0.1**2`, you should start to see some drift with `0.2**2`, and it should be quite bad with `0.5**2` or higher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZJH3QRk3mRy"
      },
      "outputs": [],
      "source": [
        "class OdometryFactorGraph:\n",
        "    def __init__(self, dt=0.1, motion_model=motion_models.Unicycle()):\n",
        "\n",
        "        self.dt = dt\n",
        "        self.motion_model = motion_model\n",
        "\n",
        "        self.prior_noise = gtsam.noiseModel.Diagonal.Sigmas(\n",
        "            np.array([0.01, 0.01, 0.01])\n",
        "        )\n",
        "        self.odometry_noise = gtsam.noiseModel.Diagonal.Sigmas(\n",
        "            1e-2 * np.ones(3)\n",
        "        )\n",
        "\n",
        "        self.graph = gtsam.NonlinearFactorGraph()\n",
        "        self.initial_estimate = gtsam.Values()\n",
        "\n",
        "        self.current_state_index = 0\n",
        "        self.poses = []\n",
        "\n",
        "    def add_motion_model_factor(self, control: np.ndarray) -> None:\n",
        "        # Compute pose at current timestep expressed in frame at previous timestep\n",
        "        # using self.motion_model.step\n",
        "        # Create a BetweenFactorPose2 between self.poses[-2] and self.poses[-1]\n",
        "        # Add that factor to self.graph\n",
        "        # Add initial estimate for self.poses[-1]\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def add_prior_pose_factor(self) -> None:\n",
        "        # Create a PriorFactorPose2 for self.poses[-1]\n",
        "        # Add that factor to self.graph\n",
        "        # Don't forget to add an initial (global) estimate for that pose!\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def solve(self):\n",
        "        # Optimize current self.graph using self.initial_estimate and params\n",
        "        # Return the result (from optimizer.optimize()) and corresponding\n",
        "        # marginals (from gtsam.Marginals(self.graph, result))\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        return result, marginals\n",
        "\n",
        "    def step(self, obs: dict, action: np.ndarray) -> dict:\n",
        "        # Each time step is called, add nodes & factors to self.graph and solve\n",
        "        # for the MLE estimate.\n",
        "\n",
        "        # To make it clear what each variable means, you can use\n",
        "        # gtsam.symbol_shorthand.X(1), ...X(2), ... for poses in GTSAM.\n",
        "\n",
        "        # Add new pose variable to factor graph, along with an initial estimate\n",
        "        self.poses.append(gtsam.symbol_shorthand.X(self.current_state_index))\n",
        "\n",
        "        # Add factors between poses (otherwise you'd just have a graph of variable nodes)\n",
        "        if self.current_state_index == 0:\n",
        "            # on the first step, just add a prior pose (no relative motion yet)\n",
        "            self.add_prior_pose_factor()\n",
        "        else:\n",
        "            # add factor for relative motion between current and last pose\n",
        "            self.add_motion_model_factor(action)\n",
        "\n",
        "        # Solve for the MLE estimate of each variable in self.graph\n",
        "        result, marginals = self.solve()\n",
        "\n",
        "        # Store the Pose2 and Covariance results in estimates\n",
        "        # Update the initial estimates for future calls to this method\n",
        "        pose_means = []\n",
        "        pose_covariances = []\n",
        "        for var in self.poses:\n",
        "            pose = result.atPose2(var)\n",
        "            pose_means.append(pose)\n",
        "            pose_covariances.append(marginals.marginalCovariance(var))\n",
        "            self.initial_estimate.update(var, pose)\n",
        "        estimates = {\n",
        "            'pose_means': pose_means,\n",
        "            'pose_covariances': pose_covariances,\n",
        "        }\n",
        "\n",
        "        self.current_state_index += 1\n",
        "        return estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a code block to see how well your `OdometryFactorGraph` is doing.\n",
        "\n",
        "You may notice that we set the `map_xwidth` and `map_ywidth` to be a little bigger than the true map (which was 10x10), so that if your pose estimates are incorrect, the scans should still get rendered without breaking the mapper's boundaries. Perhaps next year we will create a better `Mapper` whose size is not pre-defined..."
      ],
      "metadata": {
        "id": "DtHBnztFNI67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment (and set random seed so any randomness is repeatable)\n",
        "env = gymnasium.make(\"gym_neu_racing/NEUMapping-v0\")\n",
        "np.random.seed(0)\n",
        "\n",
        "# Tell the environment to use a wheel speed sensor (for odometry estimation), a\n",
        "# lidar sensor (just for mapping), and make the true state\n",
        "# available in obs (just for debugging)\n",
        "\n",
        "# You can change the variance arg in env.unwrapped.sensor_models[\"wheel_speeds\"]\n",
        "env.unwrapped.sensor_models = {\n",
        "    \"state\": sensor_models.StateFeedback(),\n",
        "    \"wheel_speeds\": sensor_models.NoisyWheelSpeedSensor(variance=0.5**2),\n",
        "    \"lidar\": sensor_models.Lidar2D(\n",
        "        env.unwrapped.map,\n",
        "        num_beams=360,\n",
        "        angle_limits=np.array([-np.pi, np.pi]),\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Set up the simulation parameters\n",
        "env.unwrapped.dt = 0.25\n",
        "env.unwrapped.motion_model = motion_models.Unicycle()\n",
        "initial_state = np.array([0.5, -3.5, np.pi / 2])\n",
        "control_policy = OpenLoopPolicy()\n",
        "\n",
        "# Reset the environment and get the first observation (state + lidar + wheel speeds)\n",
        "obs, _ = env.reset()\n",
        "env.unwrapped.state = initial_state\n",
        "obs = env.unwrapped._get_obs()  # pylint:disable=protected-access\n",
        "\n",
        "# Instantiate your OdometryFactorGraph class, which will be queried as the robot moves\n",
        "odometry = OdometryFactorGraph(dt=env.unwrapped.dt)\n",
        "\n",
        "states = []\n",
        "lidar_measurements = []\n",
        "\n",
        "for step_num, t in enumerate(np.arange(0, 40, env.unwrapped.dt)):\n",
        "\n",
        "    # calculate which action to implement on this timestep\n",
        "    action_twist = control_policy.get_action(t)\n",
        "\n",
        "    # Extract sensor data\n",
        "    states.append(obs[\"state\"])\n",
        "    lidar_measurements.append(obs[\"lidar\"])\n",
        "    measured_twist = wheel_speed_to_twist(obs[\"wheel_speeds\"])\n",
        "\n",
        "    # Update the factor graph with this step's odometry info & re-solve\n",
        "    estimates = odometry.step(obs, measured_twist)\n",
        "\n",
        "    # Update the map with latest pose estimates & lidar scans\n",
        "    if step_num % 20 == 0 and step_num > 0:\n",
        "        compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=initial_state, pause=0.1, map_xwidth=15, map_ywidth=15)\n",
        "\n",
        "    # Advance the simulator (i.e., robot moves and makes a new measurement)\n",
        "    obs, _, _, _, _ = env.step(action_twist)\n",
        "\n",
        "# Calculate a final map after the full trajectory\n",
        "compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=initial_state, pause=False, show=True, map_xwidth=15, map_ywidth=15)"
      ],
      "metadata": {
        "id": "4X8pgZVZ4u4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2c) Additionally incorporate relative pose estimates between timesteps using ICP"
      ],
      "metadata": {
        "id": "klP2vepvE0Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous part was just a warmup to get you familiar with GTSAM and working with factor graphs in code.\n",
        "\n",
        "In this part, you will replace the `BetweenPose` factors in your factor graph to be based on ICP, as opposed to propagated wheel speed measurements. But, don't throw out those wheel speeds yet! You need a good initial guess for ICP to have a chance of working well, and your propagated wheel speeds can provide this initial guess.\n",
        "\n",
        "There is a tradeoff here. On the one hand, the propagated midly noisy wheel speed measurements don't lead to a very good estimate *globally* (lots of drift), but the resulting pose estimates are at least reasonably consistent *locally*. On the other hand, ICP can either help a lot or totally destory the pose estimates. When ICP converges to a good answer, the improvements can greatly reduce the drift. But, it is not always obvious why, when, or what to do if ICP does converge to a bad answer, and adding a bad `BetweenPose` factor to your factor graph is likely to lead to completely wild answers.\n",
        "\n",
        "But, with some tuning, you should be able to get the beginnings of a decent map by the end of this part of the assignment!\n",
        "\n",
        "In case you're curious, this is still not technically SLAM, but rather a fancier odometry method than before with a mapping algorithm that is running based on that result.\n",
        "\n",
        "**Deliverables**:\n",
        "- Implement the `LidarWheelSpeedOdometry` class by completing `add_lidar_motion_model_factor`, i.e., given the current and previous lidar measurements and measured twist, add a `BetweenPose` factor between the previous and current pose, based on the output of ICP\n",
        "- Generate the same type of plots as in the previous part, for various levels of noise in the wheel speed sensor. You can keep using the perfect lidar sensor."
      ],
      "metadata": {
        "id": "_ErvKXsIFBoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LidarWheelSpeedOdometry(OdometryFactorGraph):\n",
        "    def __init__(self, dt=0.1, motion_model=motion_models.Unicycle()):\n",
        "\n",
        "        # LidarWheelSpeedOdometry will be a child class of OdometryFactorGraph from above,\n",
        "        # and will just add some methods for adding landmark nodes/factors\n",
        "        # to supplement the nodes/factors from the motion model\n",
        "        super().__init__(dt=dt, motion_model=motion_model)\n",
        "\n",
        "        self.measurement_noise = gtsam.noiseModel.Diagonal.Sigmas(\n",
        "            np.array([0.5, 0.5])\n",
        "        )\n",
        "\n",
        "        self.sensor_msgs = []\n",
        "\n",
        "    def add_lidar_motion_model_factor(\n",
        "        self, current_lidar_measurement: dict[str, np.ndarray], prev_lidar_measurement: dict[str, np.ndarray], control: np.ndarray\n",
        "    ) -> None:\n",
        "        # Use self.motion_model to get an initial guess on the transformation\n",
        "        # btwn current and prev poses\n",
        "        # Run ICP btwn current and prev lidar measurements (or vice versa)\n",
        "        # Add BetweenFactor based on ICP result\n",
        "        # Add initial estimate for self.poses[-2]\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self, obs: dict, action: np.ndarray) -> dict:\n",
        "        # Each time step is called, add nodes & factors to self.graph and solve\n",
        "        # for the MLE estimate.\n",
        "\n",
        "        # Keep track of history of sensor msgs (we'll only use the most recent\n",
        "        # one for this part, but later we'll also use this for loop closures)\n",
        "        self.sensor_msgs.append(obs)\n",
        "\n",
        "        # To make it clear what each variable means, you can use\n",
        "        # gtsam.symbol_shorthand.X(1), ...X(2), ... for states in GTSAM.\n",
        "\n",
        "        # Add new pose variable to factor graph, along with an initial estimate\n",
        "        self.poses.append(gtsam.symbol_shorthand.X(self.current_state_index))\n",
        "\n",
        "        # Add factors between poses (otherwise you'd just have a graph of variable nodes)\n",
        "        if self.current_state_index == 0:\n",
        "            # on the first step, just add a prior pose (no relative motion yet)\n",
        "            self.add_prior_pose_factor()\n",
        "        else:\n",
        "            # add factor for relative motion between current and last pose\n",
        "            self.add_lidar_motion_model_factor(\n",
        "                self.sensor_msgs[-1][\"lidar\"],\n",
        "                self.sensor_msgs[-2][\"lidar\"],\n",
        "                action,\n",
        "            )\n",
        "\n",
        "        # Solve for the MLE estimate of each variable in self.graph\n",
        "        result, marginals = self.solve()\n",
        "\n",
        "        # Store the Pose2 and Covariance results in estimates\n",
        "        # Update the initial estimates for future calls to this method\n",
        "        pose_means = []\n",
        "        pose_covariances = []\n",
        "        for var in self.poses:\n",
        "            pose = result.atPose2(var)\n",
        "            pose_means.append(pose)\n",
        "            pose_covariances.append(marginals.marginalCovariance(var))\n",
        "            self.initial_estimate.update(var, pose)\n",
        "        estimates = {\n",
        "            'pose_means': pose_means,\n",
        "            'pose_covariances': pose_covariances,\n",
        "        }\n",
        "\n",
        "        self.current_state_index += 1\n",
        "        return estimates"
      ],
      "metadata": {
        "id": "vsXmuw_k5ZNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check how much this helped your pose estimates and resulting map with this code block:"
      ],
      "metadata": {
        "id": "CQMstbzMBzly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment (and set random seed so any randomness is repeatable)\n",
        "env = gymnasium.make(\"gym_neu_racing/NEUMapping-v0\")\n",
        "np.random.seed(0)\n",
        "\n",
        "# Tell the environment to use a wheel speed and Lidar2D sensor (and make the true state available in obs, just for debugging)\n",
        "env.unwrapped.sensor_models = {\n",
        "    \"state\": sensor_models.StateFeedback(),\n",
        "    \"wheel_speeds\": sensor_models.NoisyWheelSpeedSensor(variance=0.5**2),\n",
        "    \"lidar\": sensor_models.Lidar2D(\n",
        "        env.unwrapped.map,\n",
        "        num_beams=360,\n",
        "        angle_limits=np.array([-np.pi, np.pi]),\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Set up the simulation parameters\n",
        "env.unwrapped.dt = 0.25\n",
        "env.unwrapped.motion_model = motion_models.Unicycle()\n",
        "initial_state = np.array([0.5, -3.5, np.pi / 2])\n",
        "control_policy = OpenLoopPolicy()\n",
        "\n",
        "# Reset the environment and get the first observation (state + lidar + wheel speeds)\n",
        "obs, _ = env.reset()\n",
        "env.unwrapped.state = initial_state\n",
        "obs = env.unwrapped._get_obs()  # pylint:disable=protected-access\n",
        "\n",
        "# Instantiate your LidarWheelSpeedOdometry class, which will be queried as the\n",
        "# robot moves\n",
        "lidar_wheel_speed_odometry = LidarWheelSpeedOdometry(dt=env.unwrapped.dt)\n",
        "\n",
        "states = []\n",
        "lidar_measurements = []\n",
        "\n",
        "for step_num, t in enumerate(np.arange(0, 40, env.unwrapped.dt)):\n",
        "\n",
        "    # calculate which action to implement on this timestep\n",
        "    action_twist = control_policy.get_action(t)\n",
        "\n",
        "    # Extract sensor data\n",
        "    states.append(obs[\"state\"])\n",
        "    lidar_measurements.append(obs[\"lidar\"])\n",
        "    measured_twist = wheel_speed_to_twist(obs[\"wheel_speeds\"])\n",
        "\n",
        "    # Update the factor graph with this step's odometry info & re-solve\n",
        "    estimates = lidar_wheel_speed_odometry.step(obs, measured_twist)\n",
        "\n",
        "    # Update the map with latest pose estimates & lidar scans\n",
        "    if step_num % 20 == 0 and step_num > 0:\n",
        "        compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=initial_state, pause=0.1, map_xwidth=15, map_ywidth=15)\n",
        "\n",
        "    # Advance the simulator (i.e., robot moves and makes a new measurement)\n",
        "    obs, _, _, _, _ = env.step(action_twist)\n",
        "\n",
        "# Calculate a final map after the full trajectory\n",
        "compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=initial_state, pause=False, show=True, map_xwidth=15, map_ywidth=15)"
      ],
      "metadata": {
        "id": "1NoHyf1P49Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2d) Additionally add loop closures using ICP"
      ],
      "metadata": {
        "id": "KtVITwrTCMeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The relative pose factors based on ICP should have helped a bit. But, the map probably still looks a little smeared out, because there is drift over time.\n",
        "\n",
        "To reduce this drift, you will now add some loop closures. By loop closure, we mean an additional `BetweenPose` factor that links two poses that are *not* directly subsequent in time. You can calculate the transformation parameter in this factor by running ICP on the lidar pointclouds at the two non-subsequent timesteps. Again, you'll need an initial guess, but the previous parts of the assignment have given you a few options for that. It's up to you what to use for the initial guess.\n",
        "\n",
        "Deciding when to add loop closures can get pretty complicated, so in this assignment you can simply hard-code some instances of when to add a loop closure (e.g., by defining pairs of timesteps or a rate at which to perform a loop closure). Furthermore, the term \"loop closure\" suggests the robot has left the area and come back, but you are encouraged to add \"loop closures\" more often to see if they help your estimator's performance.\n",
        "\n",
        "Again, there are some important tradeoffs here. Not having any loop closures will limit the global consistency. Whereas adding too many loop closures leads to a less sparse factor graph, which leads to higher solve times. Furthermore, just like adding a bad `BetweenPose` factor could cause issues in the previous part, the same thing can happen here. For example, if there's not enough overlap between the pointclouds at two timesteps, ICP will return a garbage transformation and the resulting bad loop closure can destroy your pose estimates. People usually err on the side of *not* adding risky loop closures for this reason!\n",
        "\n",
        "Now that we have a stored history of measurements to correct drift, there is some notion of a map, so there is an argument for calling this SLAM. It is not \"classical\" SLAM in that we are not explicitly optimizing over the map/landmarks, but the inclusion of loop closures / place recognition is one way people distinguish between Odometry and SLAM.\n",
        "\n",
        "**Deliverables**:\n",
        "- Implement `LidarSLAM`, which inherits from `LidarWheelSpeedOdometry`, by completing the `add_lidar_loop_closure_factor` method, which takes in the lidar measurements at two timesteps, the corresponding GTSAM pose variables, and an initial guess of the relative pose between those two timesteps. We provide some basic logic in `step` to add a loop closure at fixed frequencies, but you are encouraged to change this!\n",
        "- Provide some plots that show what happens for various decisions about when to add loop closures. Ideally for one or more of these plots, you get a nearly perfect map and pose estimates!\n"
      ],
      "metadata": {
        "id": "gF2u-nUYCPMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LidarSLAM(LidarWheelSpeedOdometry):\n",
        "    def __init__(self, dt=0.1, motion_model=motion_models.Unicycle()):\n",
        "\n",
        "        # LidarSLAM will be a child class of LidarWheelSpeedOdometry from above,\n",
        "        # and will just add some methods for adding loop closure factors\n",
        "        # to supplement the nodes/factors from the motion model and ICP between\n",
        "        # successive timesteps\n",
        "        super().__init__(dt=dt, motion_model=motion_model)\n",
        "\n",
        "    def add_lidar_loop_closure_factor(\n",
        "        self,\n",
        "        current_lidar_measurement: dict[str, np.ndarray],\n",
        "        prev_lidar_measurement: dict[str, np.ndarray],\n",
        "        relative_pose_initial_guess: gtsam.Pose2,\n",
        "        current_pose_var, # type: gtsam.Key\n",
        "        prev_pose_var, # type: gtsam.Key\n",
        "    ) -> None:\n",
        "\n",
        "        # Run ICP btwn current and prev lidar measurements, using relative_pose\n",
        "        # _initial_guess as initial guess\n",
        "        # Add BetweenFactor based on that result\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self, obs: dict, action: np.ndarray) -> dict:\n",
        "\n",
        "        # First do whatever LidarWheelSpeedOdometry would have done (add a node,\n",
        "        # add a prior, add an ICP factor btwn nodes, etc.). If we don't have\n",
        "        # any loop closures to add this iteration, we'll just return this.\n",
        "        estimates = super().step(obs, action)\n",
        "\n",
        "        # super().step incremeneted the time index, so temporarily decrease it\n",
        "        self.current_state_index -= 1\n",
        "\n",
        "        # Now let's add loop closure(s) if we meet certain conditions\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # increase the time index again, now that we're done with this iteration\n",
        "        self.current_state_index += 1\n",
        "        return estimates"
      ],
      "metadata": {
        "id": "Xso_OJqQy1Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can evaluate your new `LidarSLAM` code just like before:"
      ],
      "metadata": {
        "id": "FcRQBA9z0Soz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment (and set random seed so any randomness is repeatable)\n",
        "env = gymnasium.make(\"gym_neu_racing/NEUMapping-v0\")\n",
        "np.random.seed(0)\n",
        "\n",
        "# Tell the environment to use a wheel speed and Lidar2D sensor (and make the true state available in obs, just for debugging)\n",
        "env.unwrapped.sensor_models = {\n",
        "    \"state\": sensor_models.StateFeedback(),\n",
        "    \"wheel_speeds\": sensor_models.NoisyWheelSpeedSensor(variance=0.5**2),\n",
        "    \"lidar\": sensor_models.Lidar2D(\n",
        "        env.unwrapped.map,\n",
        "        num_beams=360,\n",
        "        angle_limits=np.array([-np.pi, np.pi]),\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Set up the simulation parameters\n",
        "env.unwrapped.dt = 0.25\n",
        "env.unwrapped.motion_model = motion_models.Unicycle()\n",
        "initial_state = np.array([0.5, -3.5, np.pi / 2])\n",
        "control_policy = OpenLoopPolicy()\n",
        "\n",
        "# Reset the environment and get the first observation (state + lidar + wheel speeds)\n",
        "obs, _ = env.reset()\n",
        "env.unwrapped.state = initial_state\n",
        "obs = env.unwrapped._get_obs()  # pylint:disable=protected-access\n",
        "\n",
        "# Instantiate your LidarSLAM class, which will be queried as the robot moves\n",
        "lidar_slam = LidarSLAM(dt=env.unwrapped.dt)\n",
        "\n",
        "states = []\n",
        "lidar_measurements = []\n",
        "\n",
        "for step_num, t in enumerate(np.arange(0, 40, env.unwrapped.dt)):\n",
        "\n",
        "    # calculate which action to implement on this timestep\n",
        "    action_twist = control_policy.get_action(t)\n",
        "\n",
        "    # Extract sensor data\n",
        "    states.append(obs[\"state\"])\n",
        "    lidar_measurements.append(obs[\"lidar\"])\n",
        "    measured_twist = wheel_speed_to_twist(obs[\"wheel_speeds\"])\n",
        "\n",
        "    # Update the factor graph with this step's odometry info & re-solve\n",
        "    estimates = lidar_slam.step(obs, measured_twist)\n",
        "\n",
        "    # Update the map with latest pose estimates & lidar scans\n",
        "    if step_num % 20 == 0 and step_num > 0:\n",
        "        compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=initial_state, pause=0.1, map_xwidth=15, map_ywidth=15)\n",
        "\n",
        "    # Advance the simulator (i.e., robot moves and makes a new measurement)\n",
        "    obs, _, _, _, _ = env.step(action_twist)\n",
        "\n",
        "# Calculate a final map after the full trajectory\n",
        "compute_map(estimates[\"pose_means\"], lidar_measurements, initial_state=initial_state, pause=False, show=True, map_xwidth=15, map_ywidth=15)"
      ],
      "metadata": {
        "id": "7WHnAXBs0S4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2e) [Extra Credit]: Replace the control policy to explore the map more efficiently"
      ],
      "metadata": {
        "id": "qOOrkutFxEsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, this assignment used a hard-coded, open-loop control policy to define the robot's actions at each timestep. We designed this path so the robot would get reasonable viewpoints of the map and have some opportunities for loop closures, but you likely noticed that one of the corners of the map was never explored.\n",
        "\n",
        "To get extra credit, you can implement an exploration strategy that selects actions based on the robot's current belief over its pose and the map. The objective is to reduce the uncertainty in both of these as quickly as possible, ultimately providing a complete map and accurate estimate of the robot's trajectory, using as few timesteps as possible.\n",
        "\n",
        "Since this is extra credit, we won't give more template code and will encourage you to try things out for yourself!"
      ],
      "metadata": {
        "id": "HKClXPlexKzz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-rB678P-NQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All done!"
      ],
      "metadata": {
        "id": "lsZxzl38GrrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix: [Purely Informational, not part of the assignment] Generate Pointclouds from Images for fun with ICP 😀"
      ],
      "metadata": {
        "id": "enRhfubk6HAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to generate other pointclouds to show off your ICP implementation, here's some simple code that we used to get the Northeastern logo, transform it, and save the results:"
      ],
      "metadata": {
        "id": "G57qD8DV6JHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2gnRD_zaSp1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_WbIJZYSoQs"
      },
      "outputs": [],
      "source": [
        "# Load the image\n",
        "image = cv2.imread(\"northeastern_logo.png\")\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply a binary threshold (you may need to adjust the threshold value)\n",
        "_, binary_image = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "# Extract coordinates of non-background (non-zero) pixels\n",
        "points = np.column_stack(np.where(binary_image == 255))\n",
        "\n",
        "points = np.vstack([points[:, 1], points[:, 0]]).T\n",
        "\n",
        "points[:, 1] = image.shape[1] - points[:, 1]\n",
        "\n",
        "# points now contains the (x, y) coordinates of the non-background pixels\n",
        "print(points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the points\n",
        "plt.scatter(points[:, 0], points[:, 1], color='black', s=1)  # Scatter with x,y swapped for proper display\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xZ7bDp11Sx8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = 0.9\n",
        "t = np.array([500, -70])\n",
        "T = np.array([[np.cos(theta), -np.sin(theta), t[0]], [np.sin(theta), np.cos(theta), t[1]], [0, 0, 1]])\n",
        "\n",
        "points_transformed = (T@np.hstack([points, np.ones((points.shape[0], 1))]).T).T\n",
        "\n",
        "plt.scatter(points[:, 0], points[:, 1], color='black', s=1)  # Scatter with x,y swapped for proper display\n",
        "plt.scatter(points_transformed[:, 0], points_transformed[:, 1], color='red', s=1)  # Scatter with x,y swapped for proper display\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aP6amXUFS0_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('logo_pcl2dX.txt', points_transformed)\n",
        "np.savetxt('logo_pcl2dY.txt', points)"
      ],
      "metadata": {
        "id": "MJX9M78dXlDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l5Q-SupywN0g",
        "plrBWYS7opHP",
        "TKUvYLxpv2p4",
        "enRhfubk6HAg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}